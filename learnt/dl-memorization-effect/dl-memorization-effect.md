# Deep Learning Memorization Effect

## Papers

Krueger D, Ballas N, Jastrzebski S, et al. Deep nets don't learn via memorization[J]. 2017. [[paper]](https://openreview.net/pdf?id=rJv6ZgHYg)

Zhang C, Bengio S, Hardt M, et al. Identity crisis: Memorization and generalization under extreme overparameterization[J]. arXiv preprint arXiv:1902.04698, 2019. [[paper]](https://arxiv.org/pdf/1902.04698.pdf)

Yao Q, Yang H, Han B, et al. Searching to exploit memorization effect in learning with noisy labels[C]//International Conference on Machine Learning. PMLR, 2020: 10789-10798. [[paper]](http://proceedings.mlr.press/v119/yao20b/yao20b.pdf)

Krause J, Sapp B, Howard A, et al. The unreasonable effectiveness of noisy data for fine-grained recognition[C]//European Conference on Computer Vision. Springer, Cham, 2016: 301-320. [[paper]](https://arxiv.org/pdf/1511.06789.pdf)

Arpit D, JastrzÄ™bski S, Ballas N, et al. A closer look at memorization in deep networks[C]//International Conference on Machine Learning. PMLR, 2017: 233-242. [[paper]](http://proceedings.mlr.press/v70/arpit17a/arpit17a.pdf)

Zhang C, Bengio S, Hardt M, et al. Understanding deep learning requires rethinking generalization (2016)[J]. arXiv preprint arXiv:1611.03530, 2017. [[paper]](https://arxiv.org/pdf/1611.03530.pdf)

Zhang C, Bengio S, Hardt M, et al. Understanding deep learning (still) requires rethinking generalization[J]. Communications of the ACM, 2021, 64(3): 107-115. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3446776)